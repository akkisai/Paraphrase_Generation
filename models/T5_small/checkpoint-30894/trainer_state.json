{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 30894,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006473748948015796,
      "grad_norm": 0.29584622383117676,
      "learning_rate": 0.002990580695280637,
      "loss": 0.6824,
      "step": 100
    },
    {
      "epoch": 0.012947497896031591,
      "grad_norm": 0.2504696846008301,
      "learning_rate": 0.0029808700718586133,
      "loss": 0.2693,
      "step": 200
    },
    {
      "epoch": 0.019421246844047387,
      "grad_norm": 0.19223849475383759,
      "learning_rate": 0.0029711594484365896,
      "loss": 0.2609,
      "step": 300
    },
    {
      "epoch": 0.025894995792063183,
      "grad_norm": 0.2013271003961563,
      "learning_rate": 0.002961448825014566,
      "loss": 0.2652,
      "step": 400
    },
    {
      "epoch": 0.03236874474007898,
      "grad_norm": 0.18612967431545258,
      "learning_rate": 0.0029517382015925423,
      "loss": 0.252,
      "step": 500
    },
    {
      "epoch": 0.038842493688094774,
      "grad_norm": 0.17630985379219055,
      "learning_rate": 0.0029420275781705187,
      "loss": 0.2489,
      "step": 600
    },
    {
      "epoch": 0.045316242636110574,
      "grad_norm": 0.16867630183696747,
      "learning_rate": 0.002932316954748495,
      "loss": 0.2504,
      "step": 700
    },
    {
      "epoch": 0.051789991584126366,
      "grad_norm": 0.1935388743877411,
      "learning_rate": 0.0029226063313264714,
      "loss": 0.2548,
      "step": 800
    },
    {
      "epoch": 0.058263740532142165,
      "grad_norm": 0.24368183314800262,
      "learning_rate": 0.0029128957079044477,
      "loss": 0.2438,
      "step": 900
    },
    {
      "epoch": 0.06473748948015796,
      "grad_norm": 0.21768607199192047,
      "learning_rate": 0.0029031850844824236,
      "loss": 0.2451,
      "step": 1000
    },
    {
      "epoch": 0.07121123842817376,
      "grad_norm": 0.16152046620845795,
      "learning_rate": 0.0028934744610604,
      "loss": 0.2396,
      "step": 1100
    },
    {
      "epoch": 0.07768498737618955,
      "grad_norm": 0.16466648876667023,
      "learning_rate": 0.0028837638376383763,
      "loss": 0.2503,
      "step": 1200
    },
    {
      "epoch": 0.08415873632420534,
      "grad_norm": 0.1726481169462204,
      "learning_rate": 0.0028740532142163527,
      "loss": 0.241,
      "step": 1300
    },
    {
      "epoch": 0.09063248527222115,
      "grad_norm": 0.22386345267295837,
      "learning_rate": 0.002864342590794329,
      "loss": 0.239,
      "step": 1400
    },
    {
      "epoch": 0.09710623422023694,
      "grad_norm": 0.16144539415836334,
      "learning_rate": 0.0028546319673723054,
      "loss": 0.2365,
      "step": 1500
    },
    {
      "epoch": 0.10357998316825273,
      "grad_norm": 0.20401985943317413,
      "learning_rate": 0.0028449213439502817,
      "loss": 0.2364,
      "step": 1600
    },
    {
      "epoch": 0.11005373211626854,
      "grad_norm": 0.286263644695282,
      "learning_rate": 0.002835210720528258,
      "loss": 0.2334,
      "step": 1700
    },
    {
      "epoch": 0.11652748106428433,
      "grad_norm": 0.18650349974632263,
      "learning_rate": 0.0028255000971062344,
      "loss": 0.2371,
      "step": 1800
    },
    {
      "epoch": 0.12300123001230012,
      "grad_norm": 0.22148478031158447,
      "learning_rate": 0.0028157894736842107,
      "loss": 0.2369,
      "step": 1900
    },
    {
      "epoch": 0.12947497896031593,
      "grad_norm": 0.1788540631532669,
      "learning_rate": 0.002806078850262187,
      "loss": 0.2422,
      "step": 2000
    },
    {
      "epoch": 0.1359487279083317,
      "grad_norm": 0.24006755650043488,
      "learning_rate": 0.0027963682268401634,
      "loss": 0.2396,
      "step": 2100
    },
    {
      "epoch": 0.1424224768563475,
      "grad_norm": 0.227546826004982,
      "learning_rate": 0.0027866576034181393,
      "loss": 0.2301,
      "step": 2200
    },
    {
      "epoch": 0.14889622580436332,
      "grad_norm": 0.1910189837217331,
      "learning_rate": 0.0027769469799961157,
      "loss": 0.2377,
      "step": 2300
    },
    {
      "epoch": 0.1553699747523791,
      "grad_norm": 0.21463042497634888,
      "learning_rate": 0.002767236356574092,
      "loss": 0.2306,
      "step": 2400
    },
    {
      "epoch": 0.1618437237003949,
      "grad_norm": 0.18437758088111877,
      "learning_rate": 0.0027575257331520684,
      "loss": 0.2394,
      "step": 2500
    },
    {
      "epoch": 0.16831747264841068,
      "grad_norm": 0.16308680176734924,
      "learning_rate": 0.0027478151097300447,
      "loss": 0.2299,
      "step": 2600
    },
    {
      "epoch": 0.1747912215964265,
      "grad_norm": 0.18425115942955017,
      "learning_rate": 0.002738104486308021,
      "loss": 0.2401,
      "step": 2700
    },
    {
      "epoch": 0.1812649705444423,
      "grad_norm": 0.18087135255336761,
      "learning_rate": 0.0027283938628859974,
      "loss": 0.2272,
      "step": 2800
    },
    {
      "epoch": 0.18773871949245807,
      "grad_norm": 0.23094359040260315,
      "learning_rate": 0.0027186832394639738,
      "loss": 0.2319,
      "step": 2900
    },
    {
      "epoch": 0.19421246844047388,
      "grad_norm": 0.16280104219913483,
      "learning_rate": 0.00270897261604195,
      "loss": 0.2349,
      "step": 3000
    },
    {
      "epoch": 0.20068621738848968,
      "grad_norm": 0.1908995807170868,
      "learning_rate": 0.0026992619926199265,
      "loss": 0.2365,
      "step": 3100
    },
    {
      "epoch": 0.20715996633650546,
      "grad_norm": 0.17514173686504364,
      "learning_rate": 0.002689551369197903,
      "loss": 0.23,
      "step": 3200
    },
    {
      "epoch": 0.21363371528452127,
      "grad_norm": 0.15270546078681946,
      "learning_rate": 0.002679840745775879,
      "loss": 0.2298,
      "step": 3300
    },
    {
      "epoch": 0.22010746423253708,
      "grad_norm": 0.18393920361995697,
      "learning_rate": 0.0026701301223538555,
      "loss": 0.2236,
      "step": 3400
    },
    {
      "epoch": 0.22658121318055285,
      "grad_norm": 0.17412573099136353,
      "learning_rate": 0.0026604194989318314,
      "loss": 0.2228,
      "step": 3500
    },
    {
      "epoch": 0.23305496212856866,
      "grad_norm": 0.21757325530052185,
      "learning_rate": 0.0026507088755098078,
      "loss": 0.2322,
      "step": 3600
    },
    {
      "epoch": 0.23952871107658444,
      "grad_norm": 0.16514421999454498,
      "learning_rate": 0.002640998252087784,
      "loss": 0.2375,
      "step": 3700
    },
    {
      "epoch": 0.24600246002460024,
      "grad_norm": 0.20118951797485352,
      "learning_rate": 0.0026312876286657605,
      "loss": 0.2256,
      "step": 3800
    },
    {
      "epoch": 0.252476208972616,
      "grad_norm": 0.22219185531139374,
      "learning_rate": 0.002621577005243737,
      "loss": 0.2202,
      "step": 3900
    },
    {
      "epoch": 0.25894995792063186,
      "grad_norm": 0.15494607388973236,
      "learning_rate": 0.002611866381821713,
      "loss": 0.2342,
      "step": 4000
    },
    {
      "epoch": 0.26542370686864764,
      "grad_norm": 0.1714719980955124,
      "learning_rate": 0.0026021557583996895,
      "loss": 0.2247,
      "step": 4100
    },
    {
      "epoch": 0.2718974558166634,
      "grad_norm": 0.16788837313652039,
      "learning_rate": 0.002592445134977666,
      "loss": 0.2228,
      "step": 4200
    },
    {
      "epoch": 0.27837120476467925,
      "grad_norm": 0.2328340709209442,
      "learning_rate": 0.002582734511555642,
      "loss": 0.2238,
      "step": 4300
    },
    {
      "epoch": 0.284844953712695,
      "grad_norm": 0.15530411899089813,
      "learning_rate": 0.002573023888133618,
      "loss": 0.2285,
      "step": 4400
    },
    {
      "epoch": 0.2913187026607108,
      "grad_norm": 0.16516542434692383,
      "learning_rate": 0.0025633132647115944,
      "loss": 0.2242,
      "step": 4500
    },
    {
      "epoch": 0.29779245160872664,
      "grad_norm": 0.1828245371580124,
      "learning_rate": 0.002553602641289571,
      "loss": 0.2273,
      "step": 4600
    },
    {
      "epoch": 0.3042662005567424,
      "grad_norm": 0.1732361763715744,
      "learning_rate": 0.002543892017867547,
      "loss": 0.2256,
      "step": 4700
    },
    {
      "epoch": 0.3107399495047582,
      "grad_norm": 0.1364317387342453,
      "learning_rate": 0.0025341813944455235,
      "loss": 0.2191,
      "step": 4800
    },
    {
      "epoch": 0.31721369845277403,
      "grad_norm": 0.14149734377861023,
      "learning_rate": 0.0025244707710235,
      "loss": 0.2204,
      "step": 4900
    },
    {
      "epoch": 0.3236874474007898,
      "grad_norm": 0.14052771031856537,
      "learning_rate": 0.002514760147601476,
      "loss": 0.2185,
      "step": 5000
    },
    {
      "epoch": 0.3301611963488056,
      "grad_norm": 0.20287539064884186,
      "learning_rate": 0.0025050495241794525,
      "loss": 0.2282,
      "step": 5100
    },
    {
      "epoch": 0.33663494529682136,
      "grad_norm": 0.15301311016082764,
      "learning_rate": 0.0024953389007574284,
      "loss": 0.22,
      "step": 5200
    },
    {
      "epoch": 0.3431086942448372,
      "grad_norm": 0.17333033680915833,
      "learning_rate": 0.002485628277335405,
      "loss": 0.2255,
      "step": 5300
    },
    {
      "epoch": 0.349582443192853,
      "grad_norm": 0.17605170607566833,
      "learning_rate": 0.002475917653913381,
      "loss": 0.2164,
      "step": 5400
    },
    {
      "epoch": 0.35605619214086875,
      "grad_norm": 0.1863420158624649,
      "learning_rate": 0.0024662070304913575,
      "loss": 0.222,
      "step": 5500
    },
    {
      "epoch": 0.3625299410888846,
      "grad_norm": 0.17520689964294434,
      "learning_rate": 0.002456496407069334,
      "loss": 0.2196,
      "step": 5600
    },
    {
      "epoch": 0.36900369003690037,
      "grad_norm": 0.23045800626277924,
      "learning_rate": 0.00244678578364731,
      "loss": 0.2218,
      "step": 5700
    },
    {
      "epoch": 0.37547743898491615,
      "grad_norm": 0.17691786587238312,
      "learning_rate": 0.0024370751602252865,
      "loss": 0.2273,
      "step": 5800
    },
    {
      "epoch": 0.381951187932932,
      "grad_norm": 0.165290966629982,
      "learning_rate": 0.002427364536803263,
      "loss": 0.2168,
      "step": 5900
    },
    {
      "epoch": 0.38842493688094776,
      "grad_norm": 0.1959085613489151,
      "learning_rate": 0.0024176539133812388,
      "loss": 0.2241,
      "step": 6000
    },
    {
      "epoch": 0.39489868582896354,
      "grad_norm": 0.11981161683797836,
      "learning_rate": 0.002407943289959215,
      "loss": 0.2262,
      "step": 6100
    },
    {
      "epoch": 0.40137243477697937,
      "grad_norm": 0.18154624104499817,
      "learning_rate": 0.0023982326665371915,
      "loss": 0.215,
      "step": 6200
    },
    {
      "epoch": 0.40784618372499515,
      "grad_norm": 0.13020944595336914,
      "learning_rate": 0.002388522043115168,
      "loss": 0.2112,
      "step": 6300
    },
    {
      "epoch": 0.4143199326730109,
      "grad_norm": 0.18008126318454742,
      "learning_rate": 0.002378811419693144,
      "loss": 0.2212,
      "step": 6400
    },
    {
      "epoch": 0.42079368162102676,
      "grad_norm": 0.21636544167995453,
      "learning_rate": 0.0023691007962711205,
      "loss": 0.2219,
      "step": 6500
    },
    {
      "epoch": 0.42726743056904254,
      "grad_norm": 0.1828705221414566,
      "learning_rate": 0.002359390172849097,
      "loss": 0.2282,
      "step": 6600
    },
    {
      "epoch": 0.4337411795170583,
      "grad_norm": 0.12793929874897003,
      "learning_rate": 0.002349679549427073,
      "loss": 0.2207,
      "step": 6700
    },
    {
      "epoch": 0.44021492846507415,
      "grad_norm": 0.14882434904575348,
      "learning_rate": 0.0023399689260050495,
      "loss": 0.2209,
      "step": 6800
    },
    {
      "epoch": 0.44668867741308993,
      "grad_norm": 0.17324042320251465,
      "learning_rate": 0.002330258302583026,
      "loss": 0.2204,
      "step": 6900
    },
    {
      "epoch": 0.4531624263611057,
      "grad_norm": 0.12198317050933838,
      "learning_rate": 0.0023205476791610022,
      "loss": 0.2192,
      "step": 7000
    },
    {
      "epoch": 0.4596361753091215,
      "grad_norm": 0.12376745045185089,
      "learning_rate": 0.0023108370557389786,
      "loss": 0.218,
      "step": 7100
    },
    {
      "epoch": 0.4661099242571373,
      "grad_norm": 0.17654664814472198,
      "learning_rate": 0.002301126432316955,
      "loss": 0.2191,
      "step": 7200
    },
    {
      "epoch": 0.4725836732051531,
      "grad_norm": 0.2073574960231781,
      "learning_rate": 0.002291415808894931,
      "loss": 0.216,
      "step": 7300
    },
    {
      "epoch": 0.4790574221531689,
      "grad_norm": 0.14671140909194946,
      "learning_rate": 0.002281705185472907,
      "loss": 0.2134,
      "step": 7400
    },
    {
      "epoch": 0.4855311711011847,
      "grad_norm": 0.18668587505817413,
      "learning_rate": 0.0022719945620508835,
      "loss": 0.2194,
      "step": 7500
    },
    {
      "epoch": 0.4920049200492005,
      "grad_norm": 0.20449906587600708,
      "learning_rate": 0.00226228393862886,
      "loss": 0.2165,
      "step": 7600
    },
    {
      "epoch": 0.49847866899721627,
      "grad_norm": 0.1471974104642868,
      "learning_rate": 0.0022525733152068362,
      "loss": 0.2163,
      "step": 7700
    },
    {
      "epoch": 0.504952417945232,
      "grad_norm": 0.15618953108787537,
      "learning_rate": 0.0022428626917848126,
      "loss": 0.2117,
      "step": 7800
    },
    {
      "epoch": 0.5114261668932478,
      "grad_norm": 0.18343456089496613,
      "learning_rate": 0.002233152068362789,
      "loss": 0.2154,
      "step": 7900
    },
    {
      "epoch": 0.5178999158412637,
      "grad_norm": 0.19526192545890808,
      "learning_rate": 0.0022234414449407653,
      "loss": 0.2194,
      "step": 8000
    },
    {
      "epoch": 0.5243736647892795,
      "grad_norm": 0.20121034979820251,
      "learning_rate": 0.0022137308215187416,
      "loss": 0.2099,
      "step": 8100
    },
    {
      "epoch": 0.5308474137372953,
      "grad_norm": 0.1450808048248291,
      "learning_rate": 0.002204020198096718,
      "loss": 0.2177,
      "step": 8200
    },
    {
      "epoch": 0.537321162685311,
      "grad_norm": 0.1585366129875183,
      "learning_rate": 0.0021943095746746943,
      "loss": 0.2141,
      "step": 8300
    },
    {
      "epoch": 0.5437949116333268,
      "grad_norm": 0.16755224764347076,
      "learning_rate": 0.0021845989512526707,
      "loss": 0.2047,
      "step": 8400
    },
    {
      "epoch": 0.5502686605813426,
      "grad_norm": 0.15741609036922455,
      "learning_rate": 0.0021748883278306466,
      "loss": 0.2077,
      "step": 8500
    },
    {
      "epoch": 0.5567424095293585,
      "grad_norm": 0.15363848209381104,
      "learning_rate": 0.002165177704408623,
      "loss": 0.2103,
      "step": 8600
    },
    {
      "epoch": 0.5632161584773743,
      "grad_norm": 0.26263803243637085,
      "learning_rate": 0.0021555641872208196,
      "loss": 0.2135,
      "step": 8700
    },
    {
      "epoch": 0.56968990742539,
      "grad_norm": 0.1470945179462433,
      "learning_rate": 0.002145853563798796,
      "loss": 0.2193,
      "step": 8800
    },
    {
      "epoch": 0.5761636563734058,
      "grad_norm": 0.13624587655067444,
      "learning_rate": 0.0021361429403767723,
      "loss": 0.2106,
      "step": 8900
    },
    {
      "epoch": 0.5826374053214216,
      "grad_norm": 0.18268553912639618,
      "learning_rate": 0.0021264323169547486,
      "loss": 0.2066,
      "step": 9000
    },
    {
      "epoch": 0.5891111542694374,
      "grad_norm": 0.16653892397880554,
      "learning_rate": 0.002116721693532725,
      "loss": 0.2122,
      "step": 9100
    },
    {
      "epoch": 0.5955849032174533,
      "grad_norm": 0.1399054229259491,
      "learning_rate": 0.0021070110701107013,
      "loss": 0.2163,
      "step": 9200
    },
    {
      "epoch": 0.602058652165469,
      "grad_norm": 0.1475348025560379,
      "learning_rate": 0.0020973004466886777,
      "loss": 0.2145,
      "step": 9300
    },
    {
      "epoch": 0.6085324011134848,
      "grad_norm": 0.1794150471687317,
      "learning_rate": 0.002087589823266654,
      "loss": 0.2129,
      "step": 9400
    },
    {
      "epoch": 0.6150061500615006,
      "grad_norm": 0.157000333070755,
      "learning_rate": 0.00207787919984463,
      "loss": 0.2093,
      "step": 9500
    },
    {
      "epoch": 0.6214798990095164,
      "grad_norm": 0.19805753231048584,
      "learning_rate": 0.0020681685764226063,
      "loss": 0.2103,
      "step": 9600
    },
    {
      "epoch": 0.6279536479575322,
      "grad_norm": 0.12499372661113739,
      "learning_rate": 0.0020584579530005826,
      "loss": 0.205,
      "step": 9700
    },
    {
      "epoch": 0.6344273969055481,
      "grad_norm": 0.14791817963123322,
      "learning_rate": 0.002048747329578559,
      "loss": 0.2139,
      "step": 9800
    },
    {
      "epoch": 0.6409011458535638,
      "grad_norm": 0.1731867492198944,
      "learning_rate": 0.0020390367061565353,
      "loss": 0.205,
      "step": 9900
    },
    {
      "epoch": 0.6473748948015796,
      "grad_norm": 0.17420123517513275,
      "learning_rate": 0.0020293260827345116,
      "loss": 0.209,
      "step": 10000
    },
    {
      "epoch": 0.6538486437495954,
      "grad_norm": 0.18302734196186066,
      "learning_rate": 0.002019615459312488,
      "loss": 0.2142,
      "step": 10100
    },
    {
      "epoch": 0.6603223926976112,
      "grad_norm": 0.14804136753082275,
      "learning_rate": 0.0020099048358904643,
      "loss": 0.2097,
      "step": 10200
    },
    {
      "epoch": 0.666796141645627,
      "grad_norm": 0.1601119488477707,
      "learning_rate": 0.0020001942124684407,
      "loss": 0.2111,
      "step": 10300
    },
    {
      "epoch": 0.6732698905936427,
      "grad_norm": 0.11963033676147461,
      "learning_rate": 0.001990483589046417,
      "loss": 0.2096,
      "step": 10400
    },
    {
      "epoch": 0.6797436395416586,
      "grad_norm": 0.16396784782409668,
      "learning_rate": 0.0019807729656243934,
      "loss": 0.2115,
      "step": 10500
    },
    {
      "epoch": 0.6862173884896744,
      "grad_norm": 0.1849549114704132,
      "learning_rate": 0.0019710623422023697,
      "loss": 0.2051,
      "step": 10600
    },
    {
      "epoch": 0.6926911374376902,
      "grad_norm": 0.19598284363746643,
      "learning_rate": 0.0019613517187803456,
      "loss": 0.2068,
      "step": 10700
    },
    {
      "epoch": 0.699164886385706,
      "grad_norm": 0.18541158735752106,
      "learning_rate": 0.001951641095358322,
      "loss": 0.212,
      "step": 10800
    },
    {
      "epoch": 0.7056386353337217,
      "grad_norm": 0.12831413745880127,
      "learning_rate": 0.0019419304719362983,
      "loss": 0.2017,
      "step": 10900
    },
    {
      "epoch": 0.7121123842817375,
      "grad_norm": 0.1639440357685089,
      "learning_rate": 0.0019322198485142745,
      "loss": 0.2099,
      "step": 11000
    },
    {
      "epoch": 0.7185861332297534,
      "grad_norm": 0.14337508380413055,
      "learning_rate": 0.0019225092250922508,
      "loss": 0.2089,
      "step": 11100
    },
    {
      "epoch": 0.7250598821777692,
      "grad_norm": 0.14283667504787445,
      "learning_rate": 0.0019127986016702272,
      "loss": 0.2054,
      "step": 11200
    },
    {
      "epoch": 0.731533631125785,
      "grad_norm": 0.1419944167137146,
      "learning_rate": 0.0019030879782482035,
      "loss": 0.2143,
      "step": 11300
    },
    {
      "epoch": 0.7380073800738007,
      "grad_norm": 0.13409388065338135,
      "learning_rate": 0.0018933773548261798,
      "loss": 0.2012,
      "step": 11400
    },
    {
      "epoch": 0.7444811290218165,
      "grad_norm": 0.14993661642074585,
      "learning_rate": 0.0018836667314041562,
      "loss": 0.2052,
      "step": 11500
    },
    {
      "epoch": 0.7509548779698323,
      "grad_norm": 0.13542480766773224,
      "learning_rate": 0.0018739561079821325,
      "loss": 0.211,
      "step": 11600
    },
    {
      "epoch": 0.7574286269178482,
      "grad_norm": 0.19076642394065857,
      "learning_rate": 0.0018642454845601089,
      "loss": 0.2067,
      "step": 11700
    },
    {
      "epoch": 0.763902375865864,
      "grad_norm": 0.16728094220161438,
      "learning_rate": 0.0018545348611380852,
      "loss": 0.2096,
      "step": 11800
    },
    {
      "epoch": 0.7703761248138797,
      "grad_norm": 0.17212584614753723,
      "learning_rate": 0.0018448242377160616,
      "loss": 0.2099,
      "step": 11900
    },
    {
      "epoch": 0.7768498737618955,
      "grad_norm": 0.3311915099620819,
      "learning_rate": 0.0018351136142940375,
      "loss": 0.2046,
      "step": 12000
    },
    {
      "epoch": 0.7833236227099113,
      "grad_norm": 0.133538156747818,
      "learning_rate": 0.0018254029908720138,
      "loss": 0.2128,
      "step": 12100
    },
    {
      "epoch": 0.7897973716579271,
      "grad_norm": 0.14661353826522827,
      "learning_rate": 0.0018156923674499902,
      "loss": 0.2037,
      "step": 12200
    },
    {
      "epoch": 0.7962711206059429,
      "grad_norm": 0.12554270029067993,
      "learning_rate": 0.0018059817440279665,
      "loss": 0.1973,
      "step": 12300
    },
    {
      "epoch": 0.8027448695539587,
      "grad_norm": 0.11623737961053848,
      "learning_rate": 0.0017962711206059429,
      "loss": 0.207,
      "step": 12400
    },
    {
      "epoch": 0.8092186185019745,
      "grad_norm": 0.24297703802585602,
      "learning_rate": 0.0017865604971839192,
      "loss": 0.1992,
      "step": 12500
    },
    {
      "epoch": 0.8156923674499903,
      "grad_norm": 0.15100336074829102,
      "learning_rate": 0.0017768498737618956,
      "loss": 0.1996,
      "step": 12600
    },
    {
      "epoch": 0.8221661163980061,
      "grad_norm": 0.1361081451177597,
      "learning_rate": 0.001767139250339872,
      "loss": 0.2007,
      "step": 12700
    },
    {
      "epoch": 0.8286398653460219,
      "grad_norm": 0.15338410437107086,
      "learning_rate": 0.0017574286269178483,
      "loss": 0.1983,
      "step": 12800
    },
    {
      "epoch": 0.8351136142940376,
      "grad_norm": 0.14423906803131104,
      "learning_rate": 0.0017477180034958246,
      "loss": 0.1977,
      "step": 12900
    },
    {
      "epoch": 0.8415873632420535,
      "grad_norm": 0.1184585839509964,
      "learning_rate": 0.001738007380073801,
      "loss": 0.2006,
      "step": 13000
    },
    {
      "epoch": 0.8480611121900693,
      "grad_norm": 0.16107340157032013,
      "learning_rate": 0.0017283938628859972,
      "loss": 0.2024,
      "step": 13100
    },
    {
      "epoch": 0.8545348611380851,
      "grad_norm": 0.147555872797966,
      "learning_rate": 0.0017186832394639735,
      "loss": 0.1988,
      "step": 13200
    },
    {
      "epoch": 0.8610086100861009,
      "grad_norm": 0.1509355753660202,
      "learning_rate": 0.0017089726160419499,
      "loss": 0.2022,
      "step": 13300
    },
    {
      "epoch": 0.8674823590341166,
      "grad_norm": 0.11958867311477661,
      "learning_rate": 0.0016992619926199262,
      "loss": 0.2055,
      "step": 13400
    },
    {
      "epoch": 0.8739561079821324,
      "grad_norm": 0.10886750370264053,
      "learning_rate": 0.0016895513691979026,
      "loss": 0.2006,
      "step": 13500
    },
    {
      "epoch": 0.8804298569301483,
      "grad_norm": 0.09457780420780182,
      "learning_rate": 0.001679840745775879,
      "loss": 0.2042,
      "step": 13600
    },
    {
      "epoch": 0.8869036058781641,
      "grad_norm": 0.13824905455112457,
      "learning_rate": 0.0016701301223538553,
      "loss": 0.2016,
      "step": 13700
    },
    {
      "epoch": 0.8933773548261799,
      "grad_norm": 0.13205717504024506,
      "learning_rate": 0.0016604194989318316,
      "loss": 0.1954,
      "step": 13800
    },
    {
      "epoch": 0.8998511037741956,
      "grad_norm": 0.15534156560897827,
      "learning_rate": 0.001650708875509808,
      "loss": 0.2057,
      "step": 13900
    },
    {
      "epoch": 0.9063248527222114,
      "grad_norm": 0.12966006994247437,
      "learning_rate": 0.001640998252087784,
      "loss": 0.2042,
      "step": 14000
    },
    {
      "epoch": 0.9127986016702272,
      "grad_norm": 0.14424102008342743,
      "learning_rate": 0.0016312876286657604,
      "loss": 0.2041,
      "step": 14100
    },
    {
      "epoch": 0.919272350618243,
      "grad_norm": 0.14821049571037292,
      "learning_rate": 0.0016215770052437366,
      "loss": 0.197,
      "step": 14200
    },
    {
      "epoch": 0.9257460995662589,
      "grad_norm": 0.18584157526493073,
      "learning_rate": 0.001611866381821713,
      "loss": 0.2021,
      "step": 14300
    },
    {
      "epoch": 0.9322198485142746,
      "grad_norm": 0.11627527326345444,
      "learning_rate": 0.0016021557583996893,
      "loss": 0.1974,
      "step": 14400
    },
    {
      "epoch": 0.9386935974622904,
      "grad_norm": 0.1169077455997467,
      "learning_rate": 0.0015924451349776656,
      "loss": 0.1987,
      "step": 14500
    },
    {
      "epoch": 0.9451673464103062,
      "grad_norm": 0.1384216845035553,
      "learning_rate": 0.001582734511555642,
      "loss": 0.2058,
      "step": 14600
    },
    {
      "epoch": 0.951641095358322,
      "grad_norm": 0.12450762093067169,
      "learning_rate": 0.001573023888133618,
      "loss": 0.1965,
      "step": 14700
    },
    {
      "epoch": 0.9581148443063378,
      "grad_norm": 0.10712290555238724,
      "learning_rate": 0.0015633132647115944,
      "loss": 0.2021,
      "step": 14800
    },
    {
      "epoch": 0.9645885932543536,
      "grad_norm": 0.11956217885017395,
      "learning_rate": 0.001553699747523791,
      "loss": 0.196,
      "step": 14900
    },
    {
      "epoch": 0.9710623422023694,
      "grad_norm": 0.1327476054430008,
      "learning_rate": 0.0015439891241017674,
      "loss": 0.2014,
      "step": 15000
    },
    {
      "epoch": 0.9775360911503852,
      "grad_norm": 0.1143423542380333,
      "learning_rate": 0.0015342785006797436,
      "loss": 0.1901,
      "step": 15100
    },
    {
      "epoch": 0.984009840098401,
      "grad_norm": 0.15430574119091034,
      "learning_rate": 0.00152456787725772,
      "loss": 0.2005,
      "step": 15200
    },
    {
      "epoch": 0.9904835890464168,
      "grad_norm": 0.13692286610603333,
      "learning_rate": 0.0015148572538356963,
      "loss": 0.1994,
      "step": 15300
    },
    {
      "epoch": 0.9969573379944325,
      "grad_norm": 0.0999295637011528,
      "learning_rate": 0.0015051466304136724,
      "loss": 0.198,
      "step": 15400
    },
    {
      "epoch": 1.0034310869424483,
      "grad_norm": 0.10032262653112411,
      "learning_rate": 0.001495533113225869,
      "loss": 0.184,
      "step": 15500
    },
    {
      "epoch": 1.009904835890464,
      "grad_norm": 0.1365731656551361,
      "learning_rate": 0.0014858224898038454,
      "loss": 0.1879,
      "step": 15600
    },
    {
      "epoch": 1.0163785848384799,
      "grad_norm": 0.09553132206201553,
      "learning_rate": 0.0014761118663818218,
      "loss": 0.1822,
      "step": 15700
    },
    {
      "epoch": 1.0228523337864956,
      "grad_norm": 0.12691032886505127,
      "learning_rate": 0.0014664012429597979,
      "loss": 0.1768,
      "step": 15800
    },
    {
      "epoch": 1.0293260827345116,
      "grad_norm": 0.1350720226764679,
      "learning_rate": 0.0014566906195377742,
      "loss": 0.1861,
      "step": 15900
    },
    {
      "epoch": 1.0357998316825274,
      "grad_norm": 0.14580167829990387,
      "learning_rate": 0.0014469799961157506,
      "loss": 0.1776,
      "step": 16000
    },
    {
      "epoch": 1.0422735806305432,
      "grad_norm": 0.12142828851938248,
      "learning_rate": 0.001437269372693727,
      "loss": 0.1787,
      "step": 16100
    },
    {
      "epoch": 1.048747329578559,
      "grad_norm": 0.18637274205684662,
      "learning_rate": 0.0014275587492717033,
      "loss": 0.1847,
      "step": 16200
    },
    {
      "epoch": 1.0552210785265748,
      "grad_norm": 0.13933247327804565,
      "learning_rate": 0.0014178481258496796,
      "loss": 0.1807,
      "step": 16300
    },
    {
      "epoch": 1.0616948274745905,
      "grad_norm": 0.1040138378739357,
      "learning_rate": 0.0014081375024276557,
      "loss": 0.1775,
      "step": 16400
    },
    {
      "epoch": 1.0681685764226063,
      "grad_norm": 0.12846459448337555,
      "learning_rate": 0.001398426879005632,
      "loss": 0.1803,
      "step": 16500
    },
    {
      "epoch": 1.074642325370622,
      "grad_norm": 0.1752820909023285,
      "learning_rate": 0.0013887162555836084,
      "loss": 0.1887,
      "step": 16600
    },
    {
      "epoch": 1.0811160743186379,
      "grad_norm": 0.15724186599254608,
      "learning_rate": 0.0013790056321615848,
      "loss": 0.184,
      "step": 16700
    },
    {
      "epoch": 1.0875898232666537,
      "grad_norm": 0.1399870067834854,
      "learning_rate": 0.0013692950087395611,
      "loss": 0.1866,
      "step": 16800
    },
    {
      "epoch": 1.0940635722146694,
      "grad_norm": 0.10630030930042267,
      "learning_rate": 0.0013595843853175375,
      "loss": 0.1823,
      "step": 16900
    },
    {
      "epoch": 1.1005373211626852,
      "grad_norm": 0.11691868305206299,
      "learning_rate": 0.0013498737618955136,
      "loss": 0.1829,
      "step": 17000
    },
    {
      "epoch": 1.1070110701107012,
      "grad_norm": 0.08898352086544037,
      "learning_rate": 0.00134016313847349,
      "loss": 0.1761,
      "step": 17100
    },
    {
      "epoch": 1.113484819058717,
      "grad_norm": 0.1174541786313057,
      "learning_rate": 0.0013304525150514663,
      "loss": 0.1811,
      "step": 17200
    },
    {
      "epoch": 1.1199585680067328,
      "grad_norm": 0.1591973602771759,
      "learning_rate": 0.0013207418916294426,
      "loss": 0.1867,
      "step": 17300
    },
    {
      "epoch": 1.1264323169547485,
      "grad_norm": 0.14565332233905792,
      "learning_rate": 0.001311031268207419,
      "loss": 0.1787,
      "step": 17400
    },
    {
      "epoch": 1.1329060659027643,
      "grad_norm": 0.1765672117471695,
      "learning_rate": 0.0013013206447853953,
      "loss": 0.1837,
      "step": 17500
    },
    {
      "epoch": 1.13937981485078,
      "grad_norm": 0.1328825056552887,
      "learning_rate": 0.0012916100213633717,
      "loss": 0.1791,
      "step": 17600
    },
    {
      "epoch": 1.1458535637987959,
      "grad_norm": 0.16493023931980133,
      "learning_rate": 0.0012818993979413478,
      "loss": 0.187,
      "step": 17700
    },
    {
      "epoch": 1.1523273127468117,
      "grad_norm": 0.1418386548757553,
      "learning_rate": 0.0012721887745193242,
      "loss": 0.1835,
      "step": 17800
    },
    {
      "epoch": 1.1588010616948274,
      "grad_norm": 0.13836267590522766,
      "learning_rate": 0.0012624781510973005,
      "loss": 0.1818,
      "step": 17900
    },
    {
      "epoch": 1.1652748106428432,
      "grad_norm": 0.12962649762630463,
      "learning_rate": 0.0012527675276752769,
      "loss": 0.1781,
      "step": 18000
    },
    {
      "epoch": 1.171748559590859,
      "grad_norm": 0.12691301107406616,
      "learning_rate": 0.0012430569042532532,
      "loss": 0.1856,
      "step": 18100
    },
    {
      "epoch": 1.1782223085388748,
      "grad_norm": 0.13505205512046814,
      "learning_rate": 0.0012333462808312295,
      "loss": 0.1763,
      "step": 18200
    },
    {
      "epoch": 1.1846960574868906,
      "grad_norm": 0.16153500974178314,
      "learning_rate": 0.0012236356574092057,
      "loss": 0.1831,
      "step": 18300
    },
    {
      "epoch": 1.1911698064349063,
      "grad_norm": 0.14531278610229492,
      "learning_rate": 0.001213925033987182,
      "loss": 0.1849,
      "step": 18400
    },
    {
      "epoch": 1.1976435553829223,
      "grad_norm": 0.10069994628429413,
      "learning_rate": 0.0012042144105651584,
      "loss": 0.1819,
      "step": 18500
    },
    {
      "epoch": 1.204117304330938,
      "grad_norm": 0.13539648056030273,
      "learning_rate": 0.0011945037871431347,
      "loss": 0.178,
      "step": 18600
    },
    {
      "epoch": 1.2105910532789539,
      "grad_norm": 0.132521390914917,
      "learning_rate": 0.001184793163721111,
      "loss": 0.1801,
      "step": 18700
    },
    {
      "epoch": 1.2170648022269697,
      "grad_norm": 0.11215510219335556,
      "learning_rate": 0.0011750825402990872,
      "loss": 0.1805,
      "step": 18800
    },
    {
      "epoch": 1.2235385511749854,
      "grad_norm": 0.14168143272399902,
      "learning_rate": 0.0011653719168770635,
      "loss": 0.1787,
      "step": 18900
    },
    {
      "epoch": 1.2300123001230012,
      "grad_norm": 0.18008507788181305,
      "learning_rate": 0.0011556612934550399,
      "loss": 0.1773,
      "step": 19000
    },
    {
      "epoch": 1.236486049071017,
      "grad_norm": 0.1677696704864502,
      "learning_rate": 0.001145950670033016,
      "loss": 0.1803,
      "step": 19100
    },
    {
      "epoch": 1.2429597980190328,
      "grad_norm": 0.1171029731631279,
      "learning_rate": 0.0011362400466109924,
      "loss": 0.1752,
      "step": 19200
    },
    {
      "epoch": 1.2494335469670486,
      "grad_norm": 0.15691088140010834,
      "learning_rate": 0.0011265294231889687,
      "loss": 0.1814,
      "step": 19300
    },
    {
      "epoch": 1.2559072959150643,
      "grad_norm": 0.15101508796215057,
      "learning_rate": 0.001116818799766945,
      "loss": 0.1825,
      "step": 19400
    },
    {
      "epoch": 1.2623810448630803,
      "grad_norm": 0.14422661066055298,
      "learning_rate": 0.0011071081763449214,
      "loss": 0.1791,
      "step": 19500
    },
    {
      "epoch": 1.2688547938110961,
      "grad_norm": 0.09913449734449387,
      "learning_rate": 0.0010973975529228975,
      "loss": 0.1839,
      "step": 19600
    },
    {
      "epoch": 1.275328542759112,
      "grad_norm": 0.1269237995147705,
      "learning_rate": 0.0010876869295008739,
      "loss": 0.1732,
      "step": 19700
    },
    {
      "epoch": 1.2818022917071277,
      "grad_norm": 0.0991293266415596,
      "learning_rate": 0.0010779763060788502,
      "loss": 0.1796,
      "step": 19800
    },
    {
      "epoch": 1.2882760406551435,
      "grad_norm": 0.16101953387260437,
      "learning_rate": 0.0010682656826568266,
      "loss": 0.1764,
      "step": 19900
    },
    {
      "epoch": 1.2947497896031592,
      "grad_norm": 0.1353873908519745,
      "learning_rate": 0.001058555059234803,
      "loss": 0.1792,
      "step": 20000
    },
    {
      "epoch": 1.301223538551175,
      "grad_norm": 0.11962443590164185,
      "learning_rate": 0.0010488444358127793,
      "loss": 0.1717,
      "step": 20100
    },
    {
      "epoch": 1.3076972874991908,
      "grad_norm": 0.13469929993152618,
      "learning_rate": 0.0010391338123907554,
      "loss": 0.1754,
      "step": 20200
    },
    {
      "epoch": 1.3141710364472066,
      "grad_norm": 0.13878954946994781,
      "learning_rate": 0.0010294231889687317,
      "loss": 0.1808,
      "step": 20300
    },
    {
      "epoch": 1.3206447853952223,
      "grad_norm": 0.13836577534675598,
      "learning_rate": 0.001019712565546708,
      "loss": 0.1761,
      "step": 20400
    },
    {
      "epoch": 1.3271185343432381,
      "grad_norm": 0.14620737731456757,
      "learning_rate": 0.0010100019421246844,
      "loss": 0.1779,
      "step": 20500
    },
    {
      "epoch": 1.333592283291254,
      "grad_norm": 0.1431238204240799,
      "learning_rate": 0.0010002913187026608,
      "loss": 0.1721,
      "step": 20600
    },
    {
      "epoch": 1.3400660322392697,
      "grad_norm": 0.12938392162322998,
      "learning_rate": 0.0009905806952806371,
      "loss": 0.1804,
      "step": 20700
    },
    {
      "epoch": 1.3465397811872855,
      "grad_norm": 0.11014319956302643,
      "learning_rate": 0.0009808700718586132,
      "loss": 0.1771,
      "step": 20800
    },
    {
      "epoch": 1.3530135301353012,
      "grad_norm": 0.118311807513237,
      "learning_rate": 0.0009711594484365896,
      "loss": 0.1792,
      "step": 20900
    },
    {
      "epoch": 1.359487279083317,
      "grad_norm": 0.1179518848657608,
      "learning_rate": 0.0009614488250145659,
      "loss": 0.1687,
      "step": 21000
    },
    {
      "epoch": 1.365961028031333,
      "grad_norm": 0.12157003581523895,
      "learning_rate": 0.0009517382015925423,
      "loss": 0.1772,
      "step": 21100
    },
    {
      "epoch": 1.3724347769793488,
      "grad_norm": 0.16781416535377502,
      "learning_rate": 0.0009420275781705186,
      "loss": 0.1783,
      "step": 21200
    },
    {
      "epoch": 1.3789085259273646,
      "grad_norm": 0.15448948740959167,
      "learning_rate": 0.000932316954748495,
      "loss": 0.1808,
      "step": 21300
    },
    {
      "epoch": 1.3853822748753803,
      "grad_norm": 0.1048228070139885,
      "learning_rate": 0.0009226063313264712,
      "loss": 0.1746,
      "step": 21400
    },
    {
      "epoch": 1.3918560238233961,
      "grad_norm": 0.1958429217338562,
      "learning_rate": 0.0009129928141386678,
      "loss": 0.1705,
      "step": 21500
    },
    {
      "epoch": 1.398329772771412,
      "grad_norm": 0.14620240032672882,
      "learning_rate": 0.0009032821907166441,
      "loss": 0.1725,
      "step": 21600
    },
    {
      "epoch": 1.4048035217194277,
      "grad_norm": 0.1605120599269867,
      "learning_rate": 0.0008935715672946204,
      "loss": 0.1769,
      "step": 21700
    },
    {
      "epoch": 1.4112772706674435,
      "grad_norm": 0.14864248037338257,
      "learning_rate": 0.0008838609438725966,
      "loss": 0.1767,
      "step": 21800
    },
    {
      "epoch": 1.4177510196154592,
      "grad_norm": 0.14674389362335205,
      "learning_rate": 0.000874150320450573,
      "loss": 0.1706,
      "step": 21900
    },
    {
      "epoch": 1.4242247685634752,
      "grad_norm": 0.15317107737064362,
      "learning_rate": 0.0008644396970285493,
      "loss": 0.1709,
      "step": 22000
    },
    {
      "epoch": 1.430698517511491,
      "grad_norm": 0.14731253683567047,
      "learning_rate": 0.0008547290736065255,
      "loss": 0.1801,
      "step": 22100
    },
    {
      "epoch": 1.4371722664595068,
      "grad_norm": 0.14270518720149994,
      "learning_rate": 0.0008450184501845019,
      "loss": 0.175,
      "step": 22200
    },
    {
      "epoch": 1.4436460154075226,
      "grad_norm": 0.09676492214202881,
      "learning_rate": 0.0008353078267624782,
      "loss": 0.1712,
      "step": 22300
    },
    {
      "epoch": 1.4501197643555384,
      "grad_norm": 0.12168020009994507,
      "learning_rate": 0.0008255972033404544,
      "loss": 0.1684,
      "step": 22400
    },
    {
      "epoch": 1.4565935133035541,
      "grad_norm": 0.1876925826072693,
      "learning_rate": 0.0008158865799184307,
      "loss": 0.1747,
      "step": 22500
    },
    {
      "epoch": 1.46306726225157,
      "grad_norm": 0.10429784655570984,
      "learning_rate": 0.000806175956496407,
      "loss": 0.1694,
      "step": 22600
    },
    {
      "epoch": 1.4695410111995857,
      "grad_norm": 0.13820068538188934,
      "learning_rate": 0.0007964653330743834,
      "loss": 0.1778,
      "step": 22700
    },
    {
      "epoch": 1.4760147601476015,
      "grad_norm": 0.11705164611339569,
      "learning_rate": 0.0007867547096523597,
      "loss": 0.1743,
      "step": 22800
    },
    {
      "epoch": 1.4824885090956172,
      "grad_norm": 0.1373741626739502,
      "learning_rate": 0.0007770440862303361,
      "loss": 0.1725,
      "step": 22900
    },
    {
      "epoch": 1.488962258043633,
      "grad_norm": 0.15108411014080048,
      "learning_rate": 0.0007673334628083122,
      "loss": 0.1729,
      "step": 23000
    },
    {
      "epoch": 1.4954360069916488,
      "grad_norm": 0.11507565528154373,
      "learning_rate": 0.0007576228393862886,
      "loss": 0.1747,
      "step": 23100
    },
    {
      "epoch": 1.5019097559396646,
      "grad_norm": 0.1124778464436531,
      "learning_rate": 0.0007479122159642649,
      "loss": 0.173,
      "step": 23200
    },
    {
      "epoch": 1.5083835048876804,
      "grad_norm": 0.15331268310546875,
      "learning_rate": 0.0007382015925422413,
      "loss": 0.175,
      "step": 23300
    },
    {
      "epoch": 1.5148572538356961,
      "grad_norm": 0.16348370909690857,
      "learning_rate": 0.0007284909691202176,
      "loss": 0.166,
      "step": 23400
    },
    {
      "epoch": 1.521331002783712,
      "grad_norm": 0.12361606955528259,
      "learning_rate": 0.0007187803456981938,
      "loss": 0.1711,
      "step": 23500
    },
    {
      "epoch": 1.5278047517317277,
      "grad_norm": 0.12350083142518997,
      "learning_rate": 0.0007090697222761702,
      "loss": 0.168,
      "step": 23600
    },
    {
      "epoch": 1.5342785006797435,
      "grad_norm": 0.10671687871217728,
      "learning_rate": 0.0006993590988541464,
      "loss": 0.1695,
      "step": 23700
    },
    {
      "epoch": 1.5407522496277595,
      "grad_norm": 0.17121276259422302,
      "learning_rate": 0.0006896484754321228,
      "loss": 0.1709,
      "step": 23800
    },
    {
      "epoch": 1.5472259985757753,
      "grad_norm": 0.16298073530197144,
      "learning_rate": 0.0006800349582443193,
      "loss": 0.1719,
      "step": 23900
    },
    {
      "epoch": 1.553699747523791,
      "grad_norm": 0.16248707473278046,
      "learning_rate": 0.0006703243348222957,
      "loss": 0.1692,
      "step": 24000
    },
    {
      "epoch": 1.5601734964718068,
      "grad_norm": 0.14570032060146332,
      "learning_rate": 0.0006606137114002719,
      "loss": 0.1717,
      "step": 24100
    },
    {
      "epoch": 1.5666472454198226,
      "grad_norm": 0.10298454016447067,
      "learning_rate": 0.0006509030879782483,
      "loss": 0.1703,
      "step": 24200
    },
    {
      "epoch": 1.5731209943678384,
      "grad_norm": 0.13142356276512146,
      "learning_rate": 0.0006411924645562245,
      "loss": 0.1761,
      "step": 24300
    },
    {
      "epoch": 1.5795947433158544,
      "grad_norm": 0.12484919279813766,
      "learning_rate": 0.0006314818411342007,
      "loss": 0.1714,
      "step": 24400
    },
    {
      "epoch": 1.5860684922638701,
      "grad_norm": 0.17431063950061798,
      "learning_rate": 0.0006217712177121771,
      "loss": 0.1766,
      "step": 24500
    },
    {
      "epoch": 1.592542241211886,
      "grad_norm": 0.10405495762825012,
      "learning_rate": 0.0006120605942901534,
      "loss": 0.1686,
      "step": 24600
    },
    {
      "epoch": 1.5990159901599017,
      "grad_norm": 0.2141452431678772,
      "learning_rate": 0.0006023499708681298,
      "loss": 0.1651,
      "step": 24700
    },
    {
      "epoch": 1.6054897391079175,
      "grad_norm": 0.1065247431397438,
      "learning_rate": 0.000592639347446106,
      "loss": 0.1696,
      "step": 24800
    },
    {
      "epoch": 1.6119634880559333,
      "grad_norm": 0.1280902922153473,
      "learning_rate": 0.0005829287240240824,
      "loss": 0.1698,
      "step": 24900
    },
    {
      "epoch": 1.618437237003949,
      "grad_norm": 0.12290829420089722,
      "learning_rate": 0.0005732181006020587,
      "loss": 0.173,
      "step": 25000
    },
    {
      "epoch": 1.6249109859519648,
      "grad_norm": 0.1235717236995697,
      "learning_rate": 0.0005635074771800349,
      "loss": 0.1679,
      "step": 25100
    },
    {
      "epoch": 1.6313847348999806,
      "grad_norm": 0.1181376576423645,
      "learning_rate": 0.0005537968537580113,
      "loss": 0.1716,
      "step": 25200
    },
    {
      "epoch": 1.6378584838479964,
      "grad_norm": 0.09680800139904022,
      "learning_rate": 0.0005440862303359876,
      "loss": 0.1671,
      "step": 25300
    },
    {
      "epoch": 1.6443322327960121,
      "grad_norm": 0.13181611895561218,
      "learning_rate": 0.0005343756069139639,
      "loss": 0.1661,
      "step": 25400
    },
    {
      "epoch": 1.650805981744028,
      "grad_norm": 0.10637923330068588,
      "learning_rate": 0.0005246649834919402,
      "loss": 0.1764,
      "step": 25500
    },
    {
      "epoch": 1.6572797306920437,
      "grad_norm": 0.14375156164169312,
      "learning_rate": 0.0005149543600699166,
      "loss": 0.1657,
      "step": 25600
    },
    {
      "epoch": 1.6637534796400595,
      "grad_norm": 0.12493200600147247,
      "learning_rate": 0.0005052437366478928,
      "loss": 0.1666,
      "step": 25700
    },
    {
      "epoch": 1.6702272285880753,
      "grad_norm": 0.09793170541524887,
      "learning_rate": 0.0004955331132258692,
      "loss": 0.1677,
      "step": 25800
    },
    {
      "epoch": 1.676700977536091,
      "grad_norm": 0.10764902830123901,
      "learning_rate": 0.00048591959603806566,
      "loss": 0.1732,
      "step": 25900
    },
    {
      "epoch": 1.6831747264841068,
      "grad_norm": 0.11593901365995407,
      "learning_rate": 0.00047620897261604195,
      "loss": 0.1697,
      "step": 26000
    },
    {
      "epoch": 1.6896484754321226,
      "grad_norm": 0.13880322873592377,
      "learning_rate": 0.00046649834919401824,
      "loss": 0.17,
      "step": 26100
    },
    {
      "epoch": 1.6961222243801384,
      "grad_norm": 0.11801952123641968,
      "learning_rate": 0.0004567877257719946,
      "loss": 0.1693,
      "step": 26200
    },
    {
      "epoch": 1.7025959733281544,
      "grad_norm": 0.10782577097415924,
      "learning_rate": 0.0004470771023499708,
      "loss": 0.1684,
      "step": 26300
    },
    {
      "epoch": 1.7090697222761702,
      "grad_norm": 0.09391143918037415,
      "learning_rate": 0.0004373664789279472,
      "loss": 0.1683,
      "step": 26400
    },
    {
      "epoch": 1.715543471224186,
      "grad_norm": 0.14752599596977234,
      "learning_rate": 0.0004276558555059235,
      "loss": 0.1653,
      "step": 26500
    },
    {
      "epoch": 1.7220172201722017,
      "grad_norm": 0.12868086993694305,
      "learning_rate": 0.00041794523208389976,
      "loss": 0.1639,
      "step": 26600
    },
    {
      "epoch": 1.7284909691202175,
      "grad_norm": 0.09119106829166412,
      "learning_rate": 0.0004082346086618761,
      "loss": 0.1635,
      "step": 26700
    },
    {
      "epoch": 1.7349647180682333,
      "grad_norm": 0.10637400299310684,
      "learning_rate": 0.0003985239852398524,
      "loss": 0.1692,
      "step": 26800
    },
    {
      "epoch": 1.7414384670162493,
      "grad_norm": 0.11626678705215454,
      "learning_rate": 0.00038881336181782874,
      "loss": 0.1682,
      "step": 26900
    },
    {
      "epoch": 1.747912215964265,
      "grad_norm": 0.11765338480472565,
      "learning_rate": 0.000379102738395805,
      "loss": 0.168,
      "step": 27000
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 0.11608415096998215,
      "learning_rate": 0.0003693921149737813,
      "loss": 0.168,
      "step": 27100
    },
    {
      "epoch": 1.7608597138602966,
      "grad_norm": 0.12566542625427246,
      "learning_rate": 0.0003596814915517576,
      "loss": 0.1678,
      "step": 27200
    },
    {
      "epoch": 1.7673334628083124,
      "grad_norm": 0.13641688227653503,
      "learning_rate": 0.0003499708681297339,
      "loss": 0.1683,
      "step": 27300
    },
    {
      "epoch": 1.7738072117563282,
      "grad_norm": 0.139449343085289,
      "learning_rate": 0.00034026024470771026,
      "loss": 0.1668,
      "step": 27400
    },
    {
      "epoch": 1.780280960704344,
      "grad_norm": 0.11153990775346756,
      "learning_rate": 0.00033054962128568655,
      "loss": 0.163,
      "step": 27500
    },
    {
      "epoch": 1.7867547096523597,
      "grad_norm": 0.08862127363681793,
      "learning_rate": 0.0003208389978636629,
      "loss": 0.1652,
      "step": 27600
    },
    {
      "epoch": 1.7932284586003755,
      "grad_norm": 0.11536097526550293,
      "learning_rate": 0.0003111283744416392,
      "loss": 0.1676,
      "step": 27700
    },
    {
      "epoch": 1.7997022075483913,
      "grad_norm": 0.10284338146448135,
      "learning_rate": 0.0003014177510196154,
      "loss": 0.1661,
      "step": 27800
    },
    {
      "epoch": 1.806175956496407,
      "grad_norm": 0.1559983342885971,
      "learning_rate": 0.00029170712759759177,
      "loss": 0.168,
      "step": 27900
    },
    {
      "epoch": 1.8126497054444228,
      "grad_norm": 0.10117880254983902,
      "learning_rate": 0.00028209361040978833,
      "loss": 0.1669,
      "step": 28000
    },
    {
      "epoch": 1.8191234543924386,
      "grad_norm": 0.11717873066663742,
      "learning_rate": 0.0002723829869877646,
      "loss": 0.1669,
      "step": 28100
    },
    {
      "epoch": 1.8255972033404544,
      "grad_norm": 0.11612892150878906,
      "learning_rate": 0.0002626723635657409,
      "loss": 0.164,
      "step": 28200
    },
    {
      "epoch": 1.8320709522884702,
      "grad_norm": 0.1076839491724968,
      "learning_rate": 0.0002529617401437172,
      "loss": 0.1623,
      "step": 28300
    },
    {
      "epoch": 1.838544701236486,
      "grad_norm": 0.1749512255191803,
      "learning_rate": 0.00024325111672169353,
      "loss": 0.1663,
      "step": 28400
    },
    {
      "epoch": 1.8450184501845017,
      "grad_norm": 0.11918940395116806,
      "learning_rate": 0.00023354049329966985,
      "loss": 0.1674,
      "step": 28500
    },
    {
      "epoch": 1.8514921991325175,
      "grad_norm": 0.13403263688087463,
      "learning_rate": 0.00022382986987764614,
      "loss": 0.1732,
      "step": 28600
    },
    {
      "epoch": 1.8579659480805333,
      "grad_norm": 0.14140598475933075,
      "learning_rate": 0.00021411924645562249,
      "loss": 0.1641,
      "step": 28700
    },
    {
      "epoch": 1.8644396970285493,
      "grad_norm": 0.10833155363798141,
      "learning_rate": 0.00020440862303359878,
      "loss": 0.1686,
      "step": 28800
    },
    {
      "epoch": 1.870913445976565,
      "grad_norm": 0.12569651007652283,
      "learning_rate": 0.00019469799961157504,
      "loss": 0.1651,
      "step": 28900
    },
    {
      "epoch": 1.8773871949245808,
      "grad_norm": 0.14099904894828796,
      "learning_rate": 0.00018498737618955136,
      "loss": 0.162,
      "step": 29000
    },
    {
      "epoch": 1.8838609438725966,
      "grad_norm": 0.14907455444335938,
      "learning_rate": 0.00017527675276752768,
      "loss": 0.1622,
      "step": 29100
    },
    {
      "epoch": 1.8903346928206124,
      "grad_norm": 0.14298796653747559,
      "learning_rate": 0.000165566129345504,
      "loss": 0.161,
      "step": 29200
    },
    {
      "epoch": 1.8968084417686282,
      "grad_norm": 0.10044755041599274,
      "learning_rate": 0.0001558555059234803,
      "loss": 0.1618,
      "step": 29300
    },
    {
      "epoch": 1.9032821907166442,
      "grad_norm": 0.10927695780992508,
      "learning_rate": 0.00014614488250145658,
      "loss": 0.1667,
      "step": 29400
    },
    {
      "epoch": 1.90975593966466,
      "grad_norm": 0.1134728491306305,
      "learning_rate": 0.0001364342590794329,
      "loss": 0.1626,
      "step": 29500
    },
    {
      "epoch": 1.9162296886126757,
      "grad_norm": 0.09312453866004944,
      "learning_rate": 0.00012672363565740922,
      "loss": 0.1608,
      "step": 29600
    },
    {
      "epoch": 1.9227034375606915,
      "grad_norm": 0.0950789749622345,
      "learning_rate": 0.0001170130122353855,
      "loss": 0.1662,
      "step": 29700
    },
    {
      "epoch": 1.9291771865087073,
      "grad_norm": 0.109312042593956,
      "learning_rate": 0.00010730238881336182,
      "loss": 0.1663,
      "step": 29800
    },
    {
      "epoch": 1.935650935456723,
      "grad_norm": 0.13789258897304535,
      "learning_rate": 9.759176539133813e-05,
      "loss": 0.1665,
      "step": 29900
    },
    {
      "epoch": 1.9421246844047388,
      "grad_norm": 0.09542988985776901,
      "learning_rate": 8.788114196931443e-05,
      "loss": 0.1664,
      "step": 30000
    },
    {
      "epoch": 1.9485984333527546,
      "grad_norm": 0.10893518477678299,
      "learning_rate": 7.826762478151097e-05,
      "loss": 0.1643,
      "step": 30100
    },
    {
      "epoch": 1.9550721823007704,
      "grad_norm": 0.13334496319293976,
      "learning_rate": 6.855700135948729e-05,
      "loss": 0.1676,
      "step": 30200
    },
    {
      "epoch": 1.9615459312487862,
      "grad_norm": 0.1361154317855835,
      "learning_rate": 5.8846377937463584e-05,
      "loss": 0.1642,
      "step": 30300
    },
    {
      "epoch": 1.968019680196802,
      "grad_norm": 0.10568710416555405,
      "learning_rate": 4.9135754515439896e-05,
      "loss": 0.1648,
      "step": 30400
    },
    {
      "epoch": 1.9744934291448177,
      "grad_norm": 0.15210723876953125,
      "learning_rate": 3.94251310934162e-05,
      "loss": 0.1614,
      "step": 30500
    },
    {
      "epoch": 1.9809671780928335,
      "grad_norm": 0.09686949104070663,
      "learning_rate": 2.9714507671392504e-05,
      "loss": 0.1668,
      "step": 30600
    },
    {
      "epoch": 1.9874409270408493,
      "grad_norm": 0.10475130379199982,
      "learning_rate": 2.000388424936881e-05,
      "loss": 0.1663,
      "step": 30700
    },
    {
      "epoch": 1.993914675988865,
      "grad_norm": 0.09683437645435333,
      "learning_rate": 1.0293260827345116e-05,
      "loss": 0.1581,
      "step": 30800
    }
  ],
  "logging_steps": 100,
  "max_steps": 30894,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 3125,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.689620426437427e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
