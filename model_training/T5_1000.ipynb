{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8cb4337-1a53-49f9-bf4a-4dbb3c1fda2e",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de675ed6-92ac-4ffb-88c5-a2c62cdd65fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import torch\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38b5b6-be34-472e-a7d2-c208e3089838",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2bc65-e26a-4b64-bac0-9266c71179f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"humarin/chatgpt-paraphrases\")\n",
    "dataset = dataset['train']\n",
    "dataset = dataset.filter(lambda x: x['source'] == 'quora')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9791a741-7725-43bd-85b8-8604862fe13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paraphrases(example):\n",
    "    example[\"paraphrases\"] = ast.literal_eval(example[\"paraphrases\"])\n",
    "    return example\n",
    "\n",
    "dataset1 = dataset.map(parse_paraphrases, remove_columns=['category', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfc1ade1-c917-4c50-8544-1cf1bfdae8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_paraphrase(example):\n",
    "    if example['paraphrases']:\n",
    "        example['target'] = random.choice(example['paraphrases'])\n",
    "    else:\n",
    "        example['target'] = \"\"\n",
    "    return example\n",
    "\n",
    "dataset1 = dataset1.map(select_paraphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0d5e211-bda5-47a0-b94d-f6f8d5a1600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset1.remove_columns('paraphrases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed4ff2-78d9-4403-a2cd-9062e4a2a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113db1e-7cbd-4be0-8845-dc29093c5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c7c96f-04bf-42c5-88b6-f2876c27f75c",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f01cee16-4dda-4e31-8516-cca0c72cbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset1.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset['train']\n",
    "temp_dataset = split_dataset['test'].train_test_split(test_size=0.5)\n",
    "val_dataset = temp_dataset['train']\n",
    "test_dataset = temp_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a43f04-eaf4-499c-b617-925883515b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5d19b-dee9-4faa-8868-c214879e032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9de69-ba6a-4cf3-88a4-2d0302a2aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792294de-0635-4b23-bb3d-afcb1508de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060d43a-ccaa-4c72-a4bc-1f7ebf12b9df",
   "metadata": {},
   "source": [
    "# Building a Limited Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b57e224-b176-4365-87b3-fe3ac9ddb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(dataset, n=1000):\n",
    "    text = []\n",
    "    for sample in dataset:\n",
    "        text.append(sample['text'])\n",
    "        text.append(sample['target'])\n",
    "    words = []\n",
    "    for word in text:\n",
    "        words.extend(word_tokenize(word.lower()))\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    top_words = [word for word, _ in word_counts.most_common(n)]\n",
    "\n",
    "    return top_words, word_counts\n",
    "\n",
    "top_1000_words, word_counts = top_words(train_dataset,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b48070-f8bd-4212-bac2-f56161ffb9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298ca84-2ed8-4b52-ab39-5e9f806ef1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215d498-f9d9-4c5a-bb54-ac069b3bf295",
   "metadata": {},
   "source": [
    "# Create custom T5 Tokenizor with Limited Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b98c60c-3790-4ca3-868e-50b8116d3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def create_limited_tokenizer(original_tokenizer, top_words):\n",
    "    special_tokens = [\n",
    "        original_tokenizer.pad_token,\n",
    "        original_tokenizer.eos_token,\n",
    "        original_tokenizer.bos_token,\n",
    "        original_tokenizer.unk_token,\n",
    "        original_tokenizer.sep_token,\n",
    "    ]\n",
    "\n",
    "    special_tokens = [token for token in special_tokens if token is not None]\n",
    "    \n",
    "    limited_vocab = set(special_tokens)\n",
    "    for word in top_words:\n",
    "        tokenized_word = original_tokenizer.tokenize(word)\n",
    "        limited_vocab.update(tokenized_word)\n",
    "    \n",
    "    limited_vocab_list = list(limited_vocab)\n",
    "    \n",
    "    return limited_vocab_list, original_tokenizer\n",
    "\n",
    "limited_vocab_list, limited_tokenizer = create_limited_tokenizer(original_tokenizer, top_1000_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4ac89-60ea-46e5-96de-cc1c082fad9a",
   "metadata": {},
   "source": [
    "# Preparing data for Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fec470f1-c728-4ec8-8472-a83a12a9eab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=128):\n",
    "    inputs = [\"paraphrase: \" + text for text in examples['text']]\n",
    "    targets = examples['target']\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding='max_length')\n",
    "    \n",
    "    labels_with_ignore = []\n",
    "    for label in labels['input_ids']:\n",
    "        label_with_ignore = [l if l != tokenizer.pad_token_id else -100 for l in label]\n",
    "        labels_with_ignore.append(label_with_ignore)\n",
    "    \n",
    "    model_inputs['labels'] = labels_with_ignore\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67199c35-d07b-46e0-8c8a-bbca9a7423c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_limited_vocab(examples, tokenizer, limited_vocab_list, max_length=128, prefix=\"paraphrase: \"):\n",
    "    inputs = [prefix + text for text in examples['text']]\n",
    "    targets = examples['target']\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding='max_length')\n",
    "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding='max_length')\n",
    "    \n",
    "    unk_token_id = tokenizer.unk_token_id\n",
    "    \n",
    "    model_inputs_filtered = {'input_ids': [], 'attention_mask': model_inputs['attention_mask']}\n",
    "    labels_filtered = []\n",
    "    \n",
    "    for input_ids in model_inputs['input_ids']:\n",
    "        filtered_ids = [id if tokenizer.convert_ids_to_tokens([id])[0] in limited_vocab_list \n",
    "                        else unk_token_id for id in input_ids]\n",
    "        model_inputs_filtered['input_ids'].append(filtered_ids)\n",
    "    \n",
    "    for label in labels['input_ids']:\n",
    "        filtered_label = []\n",
    "        for l in label:\n",
    "            if l == tokenizer.pad_token_id:\n",
    "                filtered_label.append(-100)  \n",
    "            elif tokenizer.convert_ids_to_tokens([l])[0] in limited_vocab_list:\n",
    "                filtered_label.append(l) \n",
    "            else:\n",
    "                filtered_label.append(unk_token_id)  \n",
    "        labels_filtered.append(filtered_label)\n",
    "    \n",
    "    model_inputs_filtered['labels'] = labels_filtered\n",
    "    return model_inputs_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c4e8b-3244-40cf-bf64-f64d1351439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_limited = train_dataset.map(\n",
    "    lambda examples: preprocess_limited_vocab(examples, limited_tokenizer, limited_vocab_list),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "validation_dataset_limited = val_dataset.map(\n",
    "    lambda examples: preprocess_limited_vocab(examples, limited_tokenizer, limited_vocab_list),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "test_dataset_limited = test_dataset.map(\n",
    "    lambda examples: preprocess_limited_vocab(examples, limited_tokenizer, limited_vocab_list),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5bb84-7044-4e0c-9ac2-25cf2197110d",
   "metadata": {},
   "source": [
    "# Fine Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08e53e-c788-43ba-907b-0d16b58013bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_small_limited_vocab\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\" \n",
    ")\n",
    "\n",
    "limited_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=limited_tokenizer,\n",
    "    model=limited_model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "limited_trainer = Trainer(\n",
    "    model=limited_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_limited,\n",
    "    eval_dataset=validation_dataset_limited,\n",
    "    tokenizer=limited_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "limited_trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3f2ef-3a37-4d00-b3a2-093eca0bb3d8",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc0c48-7f7e-47aa-ae76-6da07ca2f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_model_dir = \"./t5_small_limited_final\"\n",
    "limited_trainer.save_model(limited_model_dir)\n",
    "limited_tokenizer.save_pretrained(limited_model_dir)\n",
    "\n",
    "with open(os.path.join(limited_model_dir, \"limited_vocab_list.txt\"), \"w\") as file:\n",
    "    for word in limited_vocab_list:\n",
    "        file.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a1aa4-0a7a-4215-a37e-f1432fce949b",
   "metadata": {},
   "source": [
    "# Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92ffd066-acbf-4215-9d59-41a6e8902ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_model_dir = \"./t5_small_limited_final\"\n",
    "\n",
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(limited_model_dir)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(limited_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f456fc7-ff2f-4d80-93e4-9cb5f2ab1087",
   "metadata": {},
   "source": [
    "# Generating the paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd6672-5c01-4b1e-aa73-9ef4c55b0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_paraphrase_limited_vocab(model, tokenizer, limited_vocab_list, input_text, \n",
    "                                      max_length=64, num_beams=5):\n",
    "\n",
    "    model.eval()\n",
    "    prefix = \"paraphrase: \"\n",
    "    input_text_with_prefix = prefix + input_text\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text_with_prefix, return_tensors=\"pt\")\n",
    "    \n",
    "    unk_token_id = tokenizer.unk_token_id\n",
    "    filtered_input_ids = []\n",
    "    for id in input_ids[0]:\n",
    "        token = tokenizer.convert_ids_to_tokens([id.item()])[0]\n",
    "        if token in limited_vocab_list or id == tokenizer.pad_token_id:\n",
    "            filtered_input_ids.append(id.item())\n",
    "        else:\n",
    "            filtered_input_ids.append(unk_token_id)\n",
    "    \n",
    "    filtered_input_ids = torch.tensor([filtered_input_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            filtered_input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return paraphrased_text\n",
    "\n",
    "\n",
    "def generate_paraphrase_full_vocab(model, tokenizer, input_text, max_length=64, num_beams=5):\n",
    "    \"\"\"\n",
    "    Generate paraphrase by encoding the input text using the full/original tokenizer (no vocab restriction).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    prefix = \"paraphrase: \"\n",
    "    input_text_with_prefix = prefix + input_text\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text_with_prefix, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "\n",
    "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return paraphrased_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4542ef22-f5b1-4c8c-8db7-6a56e9eac879",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f6228-46a6-4e18-842b-203e3816e93f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In what ways does globalization exacerbate socioeconomic disparities across various geopolitical regions?\n",
      "Generated Paraphrase:what ways does global acrosst?\n",
      "\n",
      "\n",
      "Question: How does the proliferation of misinformation on social media platforms undermine democratic institutions and public discourse?\n",
      "Generated Paraphrase:does the of on social media and public?\n",
      "\n",
      "\n",
      "Question: How can sustainable urban development be achieved without compromising economic growth and infrastructural expansion?\n",
      "Generated Paraphrase:can development be without growth and inra?\n",
      "\n",
      "\n",
      "Question: What are the cognitive and psychological consequences of excessive screen time among adolescents in the digital age?\n",
      "Generated Paraphrase:are the and consequences of screen time among in the digital age?\n",
      "\n",
      "\n",
      "Question: To what extent does climate change influence the frequency and intensity of meteorological anomalies worldwide?\n",
      "Generated Paraphrase:what does change the and of?\n",
      "\n",
      "\n",
      "Question: How do multinational corporations navigate complex regulatory environments while maintaining corporate social responsibility?\n",
      "Generated Paraphrase:can while social?\n",
      "\n",
      "\n",
      "Question: In what manner does linguistic relativism affect cross-cultural communication and perception?\n",
      "Generated Paraphrase:is the impact of on- communication and?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"In what ways does globalization exacerbate socioeconomic disparities across various geopolitical regions?\",\n",
    "    \"How does the proliferation of misinformation on social media platforms undermine democratic institutions and public discourse?\",\n",
    "    \"How can sustainable urban development be achieved without compromising economic growth and infrastructural expansion?\",\n",
    "    \"What are the cognitive and psychological consequences of excessive screen time among adolescents in the digital age?\",\n",
    "    \"To what extent does climate change influence the frequency and intensity of meteorological anomalies worldwide?\",\n",
    "    \"How do multinational corporations navigate complex regulatory environments while maintaining corporate social responsibility?\",\n",
    "    \"In what manner does linguistic relativism affect cross-cultural communication and perception?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    generated_paraphrase = generate_paraphrase_limited_vocab(model,tokenizer,limited_vocab_list, question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Generated Paraphrase:{generated_paraphrase}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f7f4bb8-e696-4a53-9dff-8808e587efc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the role of in?\n",
      "does photosynthesis contribute to the growth of plants?\n"
     ]
    }
   ],
   "source": [
    "para_limited = generate_paraphrase_limited_vocab(model, tokenizer, limited_vocab_list, 'How does photosynthesis help plants grow?')\n",
    "print(para_limited)\n",
    "\n",
    "para_full = generate_paraphrase_full_vocab(model, original_tokenizer, 'How does photosynthesis help plants grow?')\n",
    "print(para_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "53f7b8ba-c95e-4d50-b223-07aa915aa3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the most city in?\n",
      "is the city of?\n"
     ]
    }
   ],
   "source": [
    "para_limited = generate_paraphrase_limited_vocab(model, tokenizer, limited_vocab_list, 'What is the captial city of india?')\n",
    "print(para_limited)\n",
    "\n",
    "para_full = generate_paraphrase_full_vocab(model, original_tokenizer, 'What is the captial city of india?')\n",
    "print(para_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d72d65-3321-4e69-8ea9-5b03803c1ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
